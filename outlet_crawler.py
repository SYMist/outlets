import time
import gspread
import os
from datetime import datetime
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- WebDriver ì„¤ì •
def setup_driver():
    options = Options()
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    driver = webdriver.Chrome(options=options)
    return driver

# --- ê°€ê²© í…ìŠ¤íŠ¸ ì²˜ë¦¬
def process_price_text(price_text):
    if "ì •ìƒê°€" in price_text and "íŒë§¤ê°€" in price_text:
        try:
            parts = price_text.split("íŒë§¤ê°€")
            original_price = parts[0].strip()
            sale_price = parts[1].strip()
            return f"<s>{original_price}</s> íŒë§¤ê°€ {sale_price}"
        except:
            return price_text
    else:
        return price_text

# --- í–‰ì‚¬ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§
def fetch_event_list(driver, branchCd, page):
    list_url = f"https://www.ehyundai.com/newPortal/SN/SN_0101000.do?branchCd={branchCd}&SN=1"
    driver.get(list_url)
    time.sleep(3)

    try:
        page_btns = driver.find_elements(By.CSS_SELECTOR, "#paging > a")
        if page <= len(page_btns):
            page_btns[page - 1].click()
            time.sleep(2)
        else:
            print(f"âš  í˜ì´ì§€ {page} ì—†ìŒ. ìŠ¤í‚µ.")
            return []
    except Exception as e:
        print(f"âŒ getContents ì‹¤í–‰ ì‹¤íŒ¨ ë˜ëŠ” ì •ì˜ë˜ì§€ ì•ŠìŒ: {e}")
        print(f"âš  í˜ì´ì§€ {page} ì—†ìŒ. ìŠ¤í‚µ.")
        return []

    soup = BeautifulSoup(driver.page_source, "html.parser")
    return soup.select("#eventList > li")

# --- í–‰ì‚¬ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§
def fetch_event_detail(driver, url):
    try:
        driver.get(url)
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, "article")))
        soup = BeautifulSoup(driver.page_source, "html.parser")

        title = soup.select_one("section.fixArea h2")
        period = soup.select_one("table.info td")

        noimg_block = soup.select("article.noImgProduct tr")
        noimg_list = [
            f"{row.find('th').text.strip()}: {row.find('td').text.strip()}"
            for row in noimg_block if row.find('th') and row.find('td')
        ]

        product_blocks = soup.select("article.twoProduct figure")
        products = []
        for p in product_blocks:
            brand = p.select_one(".p_brandNm")
            name = p.select_one(".p_productNm")
            price = p.select_one(".p_productPrc")
            img = p.select_one(".p_productImg")
            price_text = price.get_text(" ", strip=True) if price else ""

            products.append({
                "ë¸Œëœë“œ": brand.text.strip() if brand else "",
                "ì œí’ˆëª…": name.text.strip() if name else "",
                "ê°€ê²©": process_price_text(price_text),
                "ì´ë¯¸ì§€": img["src"] if img else ""
            })

        return {
            "ìƒì„¸ ì œëª©": title.text.strip() if title else "",
            "ìƒì„¸ ê¸°ê°„": period.text.strip() if period else "",
            "í…ìŠ¤íŠ¸ ì„¤ëª…": noimg_list,
            "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": products
        }

    except Exception as e:
        print(f"âŒ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return {"ìƒì„¸ ì œëª©": "", "ìƒì„¸ ê¸°ê°„": "", "í…ìŠ¤íŠ¸ ì„¤ëª…": [], "ìƒí’ˆ ë¦¬ìŠ¤íŠ¸": []}

# --- Google Sheetsì— ì—…ë¡œë“œ
def upload_to_google_sheet(sheet_title, sheet_name, new_rows):
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    CREDENTIAL_PATH = os.path.join(BASE_DIR, "credentials.json")
    today = datetime.now().strftime("%Y-%m-%d")

    creds = ServiceAccountCredentials.from_json_keyfile_name(CREDENTIAL_PATH, scope)
    client = gspread.authorize(creds)
    spreadsheet = client.open(sheet_title)

    try:
        worksheet = spreadsheet.worksheet(sheet_name)
    except gspread.exceptions.WorksheetNotFound:
        worksheet = spreadsheet.add_worksheet(title=sheet_name, rows="1000", cols="20")

    headers = ["ì œëª©", "ê¸°ê°„", "ìƒì„¸ ì œëª©", "ìƒì„¸ ê¸°ê°„", "ì¸ë„¤ì¼", "ìƒì„¸ ë§í¬", "í˜œíƒ ì„¤ëª…", "ë¸Œëœë“œ", "ì œí’ˆëª…", "ê°€ê²©", "ì´ë¯¸ì§€", "ì—…ë°ì´íŠ¸ ë‚ ì§œ"]

    try:
        existing_data = worksheet.get_all_values()
        if existing_data and existing_data[0] == headers:
            existing_data = existing_data[1:]
    except:
        existing_data = []

    existing_links = {row[5] for row in existing_data if len(row) >= 6}
    filtered_new_rows = [row + [today] for row in new_rows if len(row) >= 6 and row[5] not in existing_links]

    print(f"âœ¨ [{sheet_name}] ìƒˆë¡œ ì¶”ê°€í•  í•­ëª© ìˆ˜: {len(filtered_new_rows)}ê°œ")
    if not filtered_new_rows:
        print(f"âœ… [{sheet_name}] ì¶”ê°€í•  ë°ì´í„° ì—†ìŒ.")
        return

    all_data = [headers] + filtered_new_rows + existing_data
    worksheet.clear()
    worksheet.update('A1', all_data)

    print(f"âœ… [{sheet_name}] ì´ {len(all_data)-1}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ.")
    print(f"ğŸ”— ì‹œíŠ¸ ë§í¬: https://docs.google.com/spreadsheets/d/{spreadsheet.id}/edit")

# --- ì•„ìš¸ë › í•˜ë‚˜ í¬ë¡¤ë§
def crawl_outlet(branchCd, sheet_name):
    driver = setup_driver()
    new_rows = []

    for page in range(1, 5):
        print(f"ğŸ“„ [{sheet_name}] í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
        events = fetch_event_list(driver, branchCd, page)
        if not events:
            print(f"âš  í˜ì´ì§€ {page} ì´ë²¤íŠ¸ ì—†ìŒ")
            continue

        for event in events:
            title_tag = event.select_one(".info_tit")
            period_tag = event.select_one(".info_txt")
            img_tag = event.select_one("img")
            link_tag = event.select_one("a")

            title = title_tag.get_text(separator=" ", strip=True) if title_tag else ""
            period = period_tag.get_text(strip=True) if period_tag else ""
            image_url = img_tag["src"] if img_tag else ""
            detail_url = "https://www.ehyundai.com" + link_tag["href"] if link_tag else ""

            detail = fetch_event_detail(driver, detail_url)

            base_info = [
                title,
                period,
                detail["ìƒì„¸ ì œëª©"],
                detail["ìƒì„¸ ê¸°ê°„"],
                image_url,
                detail_url,
                " / ".join(detail["í…ìŠ¤íŠ¸ ì„¤ëª…"]),
            ]

            if detail["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]:
                for p in detail["ìƒí’ˆ ë¦¬ìŠ¤íŠ¸"]:
                    row = base_info + [p["ë¸Œëœë“œ"], p["ì œí’ˆëª…"], p["ê°€ê²©"], p["ì´ë¯¸ì§€"]]
                    new_rows.append(row)
            else:
                new_rows.append(base_info + ["", "", "", ""])

    driver.quit()
    upload_to_google_sheet("outlet-data", sheet_name, new_rows)

# --- ë©”ì¸ ì‹¤í–‰
def main():
    OUTLET_TARGETS = [
        ("B00174000", "Sheet1"),  # ì†¡ë„
        ("B00172000", "Sheet2"),  # ê¹€í¬
        ("B00178000", "Sheet3"),  # ìŠ¤í˜ì´ìŠ¤ì›
    ]

    for branchCd, sheet_name in OUTLET_TARGETS:
        crawl_outlet(branchCd, sheet_name)

    print("\nğŸ‰ ì „ì²´ ì•„ìš¸ë › í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
